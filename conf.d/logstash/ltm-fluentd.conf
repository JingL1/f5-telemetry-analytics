input {
    kafka {
        type => "kafka"
        bootstrap_servers => "kafka:9093"
        topics => [
            # "ltm-fluentd"  # need to be same as index prefix.
            "general-topic"
        ]
        decorate_events => "true"
    }
}

filter {
    json {
        # 从字符串（同样也是 json 格式）的 message 字段生成 json，字符串其他内容丢弃
        source => "message"
    }
}

filter {
    # []用来访问 json 中的字段
    if ("" in [mesg_hex]) {
        if [mesg_hex] !~ /0$/ { # 1/16
        # if [mesg_hex] !~ /[0-7]$/ { # half
        # if [mesg_hex] !~ /[0-9a-z]$/ { # all
            drop {}
        }
    }
}

filter {
    mutate {
        # 带@的是logstash默认的 field
        remove_field => [
            "@timestamp", # '%{+YYYY.MM.dd.HH}' will use @timestamp, so remove it here -- after add_field
            "message",   # comment this line for debug when looking into the original message
            "@version"
        ]
        add_field => {
            index => "${ES_INDEX}-%{+YYYY.MM.dd}"
        }
    }
}

output{
    if [stdout] == 'OK' {
        stdout { codec => rubydebug }
    }
    if [type] == "kafka" {
        elasticsearch { 
            hosts => ["elasticsearch:9200"]
            index => "%{[index]}"
        }
    }
}

